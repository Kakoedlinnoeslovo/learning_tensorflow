{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also ref: https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count [('cats', 10), ('dogs', 6), ('and', 5), ('are', 4), ('love', 3)]\n",
      "Sample data [5, 9, 10, 11, 12, 13, 5, 14, 15, 16] ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'I']\n",
      "Context pairs [[[5, 10], 9], [[9, 11], 10], [[10, 12], 11], [[11, 13], 12], [[12, 5], 13], [[13, 14], 5], [[5, 15], 14], [[14, 16], 15], [[15, 4], 16], [[16, 0], 4]]\n",
      "Skip-gram pairs [[9, 5], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 5]]\n",
      "Batches (x,y) ([0, 0, 2], [2, 24, 1])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BARCH_SIZE = 20\n",
    "#Dimension of the embedding vector. Two too small to get\n",
    "#any meaningful embeddings, but let's make it 2 for simple visualization\n",
    "EMBEDDING_SIZE = 2\n",
    "NUM_SAMPLED = 15 #Number of negative examples to sample\n",
    "\n",
    "#Sample sentences\n",
    "sentences = [\"the quick brown fox jumped over the lazy dog\",\n",
    "            \"I love cats and dogs\",\n",
    "            \"we all love cats and dogs\",\n",
    "            \"cats and dogs are great\",\n",
    "            \"sung likes cats\",\n",
    "            \"she loves dogs\",\n",
    "            \"cats can be very independent\",\n",
    "            \"cats are great companions when they want to be\",\n",
    "            \"cats are playful\",\n",
    "            \"cats are natural hunters\",\n",
    "            \"It's raining cats and dogs\",\n",
    "            \"dogs and cats love sung\"]\n",
    "\n",
    "#sentences to words and count\n",
    "words = \" \".join(sentences).split()\n",
    "count = collections.Counter(words).most_common()\n",
    "print(\"Word count\", count[:5])\n",
    "\n",
    "#Build dictionaries\n",
    "rdic = [i[0] for i in count] #reverse dic, idx -> word\n",
    "dic = {w: i for i, w in enumerate(rdic)} #dic, word -> id\n",
    "voc_size = len(dic)\n",
    "\n",
    "#Make indexed word data\n",
    "data = [dic[word] for word in words]\n",
    "print(\"Sample data\", data[:10], [rdic[t] for t in data[:10]])\n",
    "\n",
    "\n",
    "#Let's make a training data for window size 1 for simplicity\n",
    "# ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...\n",
    "\n",
    "cbow_pairs = []\n",
    "for i in range(1, len(data) - 1):\n",
    "    cbow_pairs.append([[data[i-1], data[i+1]], data[i]])\n",
    "print(\"Context pairs\", cbow_pairs[:10])\n",
    "\n",
    "#Let's make skip-gram pairs\n",
    "#(quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
    "skip_gram_pairs = []\n",
    "\n",
    "for c in cbow_pairs:\n",
    "    skip_gram_pairs.append([c[1], c[0][0]])\n",
    "    skip_gram_pairs.append([c[1], c[0][1]])\n",
    "print(\"Skip-gram pairs\", skip_gram_pairs[:10])\n",
    "\n",
    "def generate_batch(size):\n",
    "    assert size < len(skip_gram_pairs)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    r = np.random.choice(range(len(skip_gram_pairs)), \n",
    "                         size, \n",
    "                         replace=False)#without repetition\n",
    "    \n",
    "    for i in r:\n",
    "        x_data.append(skip_gram_pairs[i][0])\n",
    "        y_data.append(skip_gram_pairs[i][1])\n",
    "    return x_data, y_data\n",
    "        \n",
    "print(\"Batches (x,y)\", generate_batch (3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data\n",
    "train_inputs = tf.placeholder(tf.int32, shape = [BARCH_SIZE])\n",
    "#need to shape [batch_size, 1] for nn.nce_loss\n",
    "train_labels = tf.placeholder(tf.int32, shape = [BARCH_SIZE, 1])\n",
    "#Ops and variables to th CPU because of missing GPU implementation\n",
    "\n",
    "#What is embedding_lookup\n",
    "#Example form https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do#41922877\n",
    "\n",
    "# Yes, the purpose of tf.nn.embedding_lookup() function is to perform a lookup in the embedding matrix and return the embeddings (or in simple terms the vector representation) of words.\n",
    "\n",
    "# A simple embedding matrix (of shape: vocabulary_size x embedding_dimension) would look like below. (i.e. each word will be represented by a vector of numbers; hence the name word2vec)\n",
    "\n",
    "# Embedding Matrix\n",
    "\n",
    "# the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862\n",
    "# like 0.36808 0.20834 -0.22319 0.046283 0.20098 0.27515 -0.77127 -0.76804\n",
    "# between 0.7503 0.71623 -0.27033 0.20059 -0.17008 0.68568 -0.061672 -0.054638\n",
    "# did 0.042523 -0.21172 0.044739 -0.19248 0.26224 0.0043991 -0.88195 0.55184\n",
    "# just 0.17698 0.065221 0.28548 -0.4243 0.7499 -0.14892 -0.66786 0.11788\n",
    "# national -1.1105 0.94945 -0.17078 0.93037 -0.2477 -0.70633 -0.8649 -0.56118\n",
    "# day 0.11626 0.53897 -0.39514 -0.26027 0.57706 -0.79198 -0.88374 0.30119\n",
    "# country -0.13531 0.15485 -0.07309 0.034013 -0.054457 -0.20541 -0.60086 -0.22407\n",
    "# under 0.13721 -0.295 -0.05916 -0.59235 0.02301 0.21884 -0.34254 -0.70213\n",
    "# such 0.61012 0.33512 -0.53499 0.36139 -0.39866 0.70627 -0.18699 -0.77246\n",
    "# second -0.29809 0.28069 0.087102 0.54455 0.70003 0.44778 -0.72565 0.62309 \n",
    "# I split the above embedding matrix and loaded only the words in vocab which will be our vocabulary and the corresponding vectors in emb array.\n",
    "\n",
    "# vocab = ['the','like','between','did','just','national','day','country','under','such','second']\n",
    "\n",
    "# emb = np.array([[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862],\n",
    "#    [0.36808, 0.20834, -0.22319, 0.046283, 0.20098, 0.27515, -0.77127, -0.76804],\n",
    "#    [0.7503, 0.71623, -0.27033, 0.20059, -0.17008, 0.68568, -0.061672, -0.054638],\n",
    "#    [0.042523, -0.21172, 0.044739, -0.19248, 0.26224, 0.0043991, -0.88195, 0.55184],\n",
    "#    [0.17698, 0.065221, 0.28548, -0.4243, 0.7499, -0.14892, -0.66786, 0.11788],\n",
    "#    [-1.1105, 0.94945, -0.17078, 0.93037, -0.2477, -0.70633, -0.8649, -0.56118],\n",
    "#    [0.11626, 0.53897, -0.39514, -0.26027, 0.57706, -0.79198, -0.88374, 0.30119],\n",
    "#    [-0.13531, 0.15485, -0.07309, 0.034013, -0.054457, -0.20541, -0.60086, -0.22407],\n",
    "#    [ 0.13721, -0.295, -0.05916, -0.59235, 0.02301, 0.21884, -0.34254, -0.70213],\n",
    "#    [ 0.61012, 0.33512, -0.53499, 0.36139, -0.39866, 0.70627, -0.18699, -0.77246 ],\n",
    "#    [ -0.29809, 0.28069, 0.087102, 0.54455, 0.70003, 0.44778, -0.72565, 0.62309 ]])\n",
    "\n",
    "\n",
    "# emb.shape\n",
    "# # (11, 8)\n",
    "\n",
    "\n",
    "\n",
    "# In [54]: from collections import OrderedDict\n",
    "\n",
    "# # embedding as TF tensor (for now constant; could be tf.Variable() during training)\n",
    "# In [55]: tf_embedding = tf.constant(emb, dtype=tf.float32)\n",
    "\n",
    "# # input for which we need the embedding\n",
    "# In [56]: input_str = \"like the country\"\n",
    "\n",
    "# # build index based on our `vocabulary`\n",
    "# In [57]: word_to_idx = OrderedDict({w:vocab.index(w) for w in input_str.split() if w in vocab})\n",
    "\n",
    "# # lookup in embedding matrix & return the vectors for the input words\n",
    "# In [58]: tf.nn.embedding_lookup(tf_embedding, list(word_to_idx.values())).eval()\n",
    "# Out[58]: \n",
    "# array([[ 0.36807999,  0.20834   , -0.22318999,  0.046283  ,  0.20097999,\n",
    "#          0.27515   , -0.77126998, -0.76804   ],\n",
    "#        [ 0.41800001,  0.24968   , -0.41242   ,  0.1217    ,  0.34527001,\n",
    "#         -0.044457  , -0.49687999, -0.17862   ],\n",
    "#        [-0.13530999,  0.15485001, -0.07309   ,  0.034013  , -0.054457  ,\n",
    "#         -0.20541   , -0.60086   , -0.22407   ]], dtype=float32)\n",
    "\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    #Look up embeddings for inputs\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([voc_size, EMBEDDING_SIZE], -1.0, 1.0 ))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs) #lookup table\n",
    "    \n",
    "#Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "        tf.random_uniform([voc_size, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "#Compute the average NCE loss for the batch.\n",
    "#This does the magic:\n",
    "# tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes ...)\n",
    "#It automatically draws negative samples when we evaluate the loss.\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, \n",
    "                                     nce_biases, \n",
    "                                     train_labels, \n",
    "                                     embed, \n",
    "                                     NUM_SAMPLED, \n",
    "                                     voc_size))\n",
    "#Use the adam optimizer\n",
    "train_op = tf.train.AdamOptimizer(1e-1).minimize(loss)\n",
    "\n",
    "\n",
    "#Why is nce?\n",
    "#https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss\n",
    "\n",
    "#Taken from this post:https://stats.stackexchange.com/a/245452/154812\n",
    "\n",
    "# The issue\n",
    "\n",
    "# There are some issues with learning the word vectors using an \n",
    "#\"standard\" neural network. \n",
    "#In this way, the word vectors are learned while the network \n",
    "#learns to predict the next word given a window of words \n",
    "#(the input of the network).\n",
    "\n",
    "# Predicting the next word is like predicting the class. \n",
    "#That is, such a network is just a \"standard\" multinomial (multi-class) \n",
    "#classifier. And this network must have as many output neurons \n",
    "#as classes there are. When classes are actual words, \n",
    "#the number of neurons is, well, huge.\n",
    "\n",
    "# A \"standard\" neural network is usually trained with a \n",
    "#cross-entropy cost function which requires the values of the \n",
    "#output neurons to represent probabilities - which means that the \n",
    "#output \"scores\" computed by the network for each class have to be \n",
    "#normalized, converted into actual probabilities for each class. \n",
    "#This normalization step is achieved by means of the softmax function. \n",
    "#Softmax is very costly when applied to a huge output layer.\n",
    "\n",
    "# The (a) solution\n",
    "\n",
    "# In order to deal with this issue, that is, \n",
    "#the expensive computation of the softmax, Word2Vec uses a technique \n",
    "#called noise-contrastive estimation. This technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E] to learn word embeddings from unlabelled natural language text.\n",
    "\n",
    "# The basic idea is to convert a multinomial classification problem \n",
    "#(as it is the problem of predicting the next word) to a binary \n",
    "#classification problem. That is, instead of using softmax to estimate \n",
    "#a true probability distribution of the output word, a binary logistic \n",
    "#regression (binary classification) is used instead.\n",
    "\n",
    "# For each training sample, the enhanced (optimized) classifier \n",
    "#is fed a true pair (a center word and another word that appears \n",
    "#in its context) and a number of kk randomly \n",
    "#corrupted pairs (consisting of the center word and a randomly \n",
    "#chosen word from the vocabulary). By learning to distinguish the \n",
    "#true pairs from corrupted ones, the classifier will ultimately \n",
    "#learn the word vectors.\n",
    "\n",
    "# This is important: instead of predicting the next word \n",
    "#(the \"standard\" training technique), the optimized classifier \n",
    "#simply predicts whether a pair of words is good or bad.\n",
    "\n",
    "# Word2Vec slightly customizes the process and calls it negative \n",
    "#sampling. In Word2Vec, the words for the negative samples \n",
    "#(used for the corrupted pairs) are drawn from a specially designed \n",
    "#distribution, which favours less frequent words to be drawn more often.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launch the graph in a session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
