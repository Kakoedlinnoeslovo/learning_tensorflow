{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also ref: https://www.tensorflow.org/tutorials/representation/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count [('cats', 10), ('dogs', 6), ('and', 5), ('are', 4), ('love', 3)]\n",
      "Sample data [5, 9, 10, 11, 12, 13, 5, 14, 15, 16] ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'I']\n",
      "Context pairs [[[5, 10], 9], [[9, 11], 10], [[10, 12], 11], [[11, 13], 12], [[12, 5], 13], [[13, 14], 5], [[5, 15], 14], [[14, 16], 15], [[15, 4], 16], [[16, 0], 4]]\n",
      "Skip-gram pairs [[9, 5], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 5]]\n",
      "Batches (x,y) ([0, 3, 0], [2, 6, 34])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "#Dimension of the embedding vector. Two too small to get\n",
    "#any meaningful embeddings, but let's make it 2 for simple visualization\n",
    "EMBEDDING_SIZE = 2\n",
    "NUM_SAMPLED = 15 #Number of negative examples to sample\n",
    "\n",
    "#Sample sentences\n",
    "sentences = [\"the quick brown fox jumped over the lazy dog\",\n",
    "            \"I love cats and dogs\",\n",
    "            \"we all love cats and dogs\",\n",
    "            \"cats and dogs are great\",\n",
    "            \"sung likes cats\",\n",
    "            \"she loves dogs\",\n",
    "            \"cats can be very independent\",\n",
    "            \"cats are great companions when they want to be\",\n",
    "            \"cats are playful\",\n",
    "            \"cats are natural hunters\",\n",
    "            \"It's raining cats and dogs\",\n",
    "            \"dogs and cats love sung\"]\n",
    "\n",
    "#sentences to words and count\n",
    "words = \" \".join(sentences).split()\n",
    "count = collections.Counter(words).most_common()\n",
    "print(\"Word count\", count[:5])\n",
    "\n",
    "#Build dictionaries\n",
    "rdic = [i[0] for i in count] #reverse dic, idx -> word\n",
    "dic = {w: i for i, w in enumerate(rdic)} #dic, word -> id\n",
    "voc_size = len(dic)\n",
    "\n",
    "#Make indexed word data\n",
    "data = [dic[word] for word in words]\n",
    "print(\"Sample data\", data[:10], [rdic[t] for t in data[:10]])\n",
    "\n",
    "\n",
    "#Let's make a training data for window size 1 for simplicity\n",
    "# ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...\n",
    "\n",
    "cbow_pairs = []\n",
    "for i in range(1, len(data) - 1):\n",
    "    cbow_pairs.append([[data[i-1], data[i+1]], data[i]])\n",
    "print(\"Context pairs\", cbow_pairs[:10])\n",
    "\n",
    "#Let's make skip-gram pairs\n",
    "#(quick, the), (quick, brown), (brown, quick), (brown, fox), ...\n",
    "skip_gram_pairs = []\n",
    "\n",
    "for c in cbow_pairs:\n",
    "    skip_gram_pairs.append([c[1], c[0][0]])\n",
    "    skip_gram_pairs.append([c[1], c[0][1]])\n",
    "print(\"Skip-gram pairs\", skip_gram_pairs[:10])\n",
    "\n",
    "def generate_batch(size):\n",
    "    assert size < len(skip_gram_pairs)\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    r = np.random.choice(range(len(skip_gram_pairs)), \n",
    "                         size, \n",
    "                         replace=False)#without repetition\n",
    "    \n",
    "    for i in r:\n",
    "        x_data.append(skip_gram_pairs[i][0])\n",
    "        y_data.append(skip_gram_pairs[i][1])\n",
    "    return x_data, y_data\n",
    "        \n",
    "print(\"Batches (x,y)\", generate_batch (3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data\n",
    "train_inputs = tf.placeholder(tf.int32, shape = [BATCH_SIZE])\n",
    "#need to shape [batch_size, 1] for nn.nce_loss\n",
    "train_labels = tf.placeholder(tf.int32, shape = [BATCH_SIZE, 1])\n",
    "#Ops and variables to th CPU because of missing GPU implementation\n",
    "\n",
    "#What is embedding_lookup\n",
    "#Example form https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do#41922877\n",
    "\n",
    "# Yes, the purpose of tf.nn.embedding_lookup() function is to perform a lookup in the embedding matrix and return the embeddings (or in simple terms the vector representation) of words.\n",
    "\n",
    "# A simple embedding matrix (of shape: vocabulary_size x embedding_dimension) would look like below. (i.e. each word will be represented by a vector of numbers; hence the name word2vec)\n",
    "\n",
    "# Embedding Matrix\n",
    "\n",
    "# the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862\n",
    "# like 0.36808 0.20834 -0.22319 0.046283 0.20098 0.27515 -0.77127 -0.76804\n",
    "# between 0.7503 0.71623 -0.27033 0.20059 -0.17008 0.68568 -0.061672 -0.054638\n",
    "# did 0.042523 -0.21172 0.044739 -0.19248 0.26224 0.0043991 -0.88195 0.55184\n",
    "# just 0.17698 0.065221 0.28548 -0.4243 0.7499 -0.14892 -0.66786 0.11788\n",
    "# national -1.1105 0.94945 -0.17078 0.93037 -0.2477 -0.70633 -0.8649 -0.56118\n",
    "# day 0.11626 0.53897 -0.39514 -0.26027 0.57706 -0.79198 -0.88374 0.30119\n",
    "# country -0.13531 0.15485 -0.07309 0.034013 -0.054457 -0.20541 -0.60086 -0.22407\n",
    "# under 0.13721 -0.295 -0.05916 -0.59235 0.02301 0.21884 -0.34254 -0.70213\n",
    "# such 0.61012 0.33512 -0.53499 0.36139 -0.39866 0.70627 -0.18699 -0.77246\n",
    "# second -0.29809 0.28069 0.087102 0.54455 0.70003 0.44778 -0.72565 0.62309 \n",
    "# I split the above embedding matrix and loaded only the words in vocab which will be our vocabulary and the corresponding vectors in emb array.\n",
    "\n",
    "# vocab = ['the','like','between','did','just','national','day','country','under','such','second']\n",
    "\n",
    "# emb = np.array([[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862],\n",
    "#    [0.36808, 0.20834, -0.22319, 0.046283, 0.20098, 0.27515, -0.77127, -0.76804],\n",
    "#    [0.7503, 0.71623, -0.27033, 0.20059, -0.17008, 0.68568, -0.061672, -0.054638],\n",
    "#    [0.042523, -0.21172, 0.044739, -0.19248, 0.26224, 0.0043991, -0.88195, 0.55184],\n",
    "#    [0.17698, 0.065221, 0.28548, -0.4243, 0.7499, -0.14892, -0.66786, 0.11788],\n",
    "#    [-1.1105, 0.94945, -0.17078, 0.93037, -0.2477, -0.70633, -0.8649, -0.56118],\n",
    "#    [0.11626, 0.53897, -0.39514, -0.26027, 0.57706, -0.79198, -0.88374, 0.30119],\n",
    "#    [-0.13531, 0.15485, -0.07309, 0.034013, -0.054457, -0.20541, -0.60086, -0.22407],\n",
    "#    [ 0.13721, -0.295, -0.05916, -0.59235, 0.02301, 0.21884, -0.34254, -0.70213],\n",
    "#    [ 0.61012, 0.33512, -0.53499, 0.36139, -0.39866, 0.70627, -0.18699, -0.77246 ],\n",
    "#    [ -0.29809, 0.28069, 0.087102, 0.54455, 0.70003, 0.44778, -0.72565, 0.62309 ]])\n",
    "\n",
    "\n",
    "# emb.shape\n",
    "# # (11, 8)\n",
    "\n",
    "\n",
    "\n",
    "# In [54]: from collections import OrderedDict\n",
    "\n",
    "# # embedding as TF tensor (for now constant; could be tf.Variable() during training)\n",
    "# In [55]: tf_embedding = tf.constant(emb, dtype=tf.float32)\n",
    "\n",
    "# # input for which we need the embedding\n",
    "# In [56]: input_str = \"like the country\"\n",
    "\n",
    "# # build index based on our `vocabulary`\n",
    "# In [57]: word_to_idx = OrderedDict({w:vocab.index(w) for w in input_str.split() if w in vocab})\n",
    "\n",
    "# # lookup in embedding matrix & return the vectors for the input words\n",
    "# In [58]: tf.nn.embedding_lookup(tf_embedding, list(word_to_idx.values())).eval()\n",
    "# Out[58]: \n",
    "# array([[ 0.36807999,  0.20834   , -0.22318999,  0.046283  ,  0.20097999,\n",
    "#          0.27515   , -0.77126998, -0.76804   ],\n",
    "#        [ 0.41800001,  0.24968   , -0.41242   ,  0.1217    ,  0.34527001,\n",
    "#         -0.044457  , -0.49687999, -0.17862   ],\n",
    "#        [-0.13530999,  0.15485001, -0.07309   ,  0.034013  , -0.054457  ,\n",
    "#         -0.20541   , -0.60086   , -0.22407   ]], dtype=float32)\n",
    "\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    #Look up embeddings for inputs\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([voc_size, EMBEDDING_SIZE], -1.0, 1.0 ))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs) #lookup table\n",
    "    \n",
    "#Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "        tf.random_uniform([voc_size, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
    "\n",
    "#Compute the average NCE loss for the batch.\n",
    "#This does the magic:\n",
    "# tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes ...)\n",
    "#It automatically draws negative samples when we evaluate the loss.\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, \n",
    "                                     nce_biases, \n",
    "                                     train_labels, \n",
    "                                     embed, \n",
    "                                     NUM_SAMPLED, \n",
    "                                     voc_size))\n",
    "#Use the adam optimizer\n",
    "train_op = tf.train.AdamOptimizer(1e-1).minimize(loss)\n",
    "\n",
    "\n",
    "#Why is nce?\n",
    "#https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss\n",
    "\n",
    "#Taken from this post:https://stats.stackexchange.com/a/245452/154812\n",
    "\n",
    "# The issue\n",
    "\n",
    "# There are some issues with learning the word vectors using an \n",
    "#\"standard\" neural network. \n",
    "#In this way, the word vectors are learned while the network \n",
    "#learns to predict the next word given a window of words \n",
    "#(the input of the network).\n",
    "\n",
    "# Predicting the next word is like predicting the class. \n",
    "#That is, such a network is just a \"standard\" multinomial (multi-class) \n",
    "#classifier. And this network must have as many output neurons \n",
    "#as classes there are. When classes are actual words, \n",
    "#the number of neurons is, well, huge.\n",
    "\n",
    "# A \"standard\" neural network is usually trained with a \n",
    "#cross-entropy cost function which requires the values of the \n",
    "#output neurons to represent probabilities - which means that the \n",
    "#output \"scores\" computed by the network for each class have to be \n",
    "#normalized, converted into actual probabilities for each class. \n",
    "#This normalization step is achieved by means of the softmax function. \n",
    "#Softmax is very costly when applied to a huge output layer.\n",
    "\n",
    "# The (a) solution\n",
    "\n",
    "# In order to deal with this issue, that is, \n",
    "#the expensive computation of the softmax, Word2Vec uses a technique \n",
    "#called noise-contrastive estimation. This technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E] to learn word embeddings from unlabelled natural language text.\n",
    "\n",
    "# The basic idea is to convert a multinomial classification problem \n",
    "#(as it is the problem of predicting the next word) to a binary \n",
    "#classification problem. That is, instead of using softmax to estimate \n",
    "#a true probability distribution of the output word, a binary logistic \n",
    "#regression (binary classification) is used instead.\n",
    "\n",
    "# For each training sample, the enhanced (optimized) classifier \n",
    "#is fed a true pair (a center word and another word that appears \n",
    "#in its context) and a number of kk randomly \n",
    "#corrupted pairs (consisting of the center word and a randomly \n",
    "#chosen word from the vocabulary). By learning to distinguish the \n",
    "#true pairs from corrupted ones, the classifier will ultimately \n",
    "#learn the word vectors.\n",
    "\n",
    "# This is important: instead of predicting the next word \n",
    "#(the \"standard\" training technique), the optimized classifier \n",
    "#simply predicts whether a pair of words is good or bad.\n",
    "\n",
    "# Word2Vec slightly customizes the process and calls it negative \n",
    "#sampling. In Word2Vec, the words for the negative samples \n",
    "#(used for the corrupted pairs) are drawn from a specially designed \n",
    "#distribution, which favours less frequent words to be drawn more often.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at  0 17.497435\n",
      "Loss at  10 10.293896\n",
      "Loss at  20 7.468894\n",
      "Loss at  30 4.035839\n",
      "Loss at  40 3.8947816\n",
      "Loss at  50 3.6375897\n",
      "Loss at  60 3.1234982\n",
      "Loss at  70 3.5265267\n",
      "Loss at  80 3.037312\n",
      "Loss at  90 3.5179744\n",
      "Loss at  100 3.7563534\n",
      "Loss at  110 3.110825\n",
      "Loss at  120 3.3392868\n",
      "Loss at  130 3.3702388\n",
      "Loss at  140 3.28947\n",
      "Loss at  150 3.2387931\n",
      "Loss at  160 3.29982\n",
      "Loss at  170 3.3454013\n",
      "Loss at  180 3.209456\n",
      "Loss at  190 3.0865238\n",
      "Loss at  200 3.3438728\n",
      "Loss at  210 3.2444801\n",
      "Loss at  220 3.2209086\n",
      "Loss at  230 3.0621705\n",
      "Loss at  240 2.980598\n",
      "Loss at  250 3.1902223\n",
      "Loss at  260 2.9946637\n",
      "Loss at  270 2.6602054\n",
      "Loss at  280 2.8156724\n",
      "Loss at  290 3.0276017\n",
      "Loss at  300 2.994828\n",
      "Loss at  310 3.1773057\n",
      "Loss at  320 3.1370134\n",
      "Loss at  330 3.1535673\n",
      "Loss at  340 3.1349244\n",
      "Loss at  350 3.441119\n",
      "Loss at  360 3.2270844\n",
      "Loss at  370 3.0532537\n",
      "Loss at  380 3.3765454\n",
      "Loss at  390 3.1100736\n",
      "Loss at  400 2.9533908\n",
      "Loss at  410 3.1333275\n",
      "Loss at  420 3.0015223\n",
      "Loss at  430 2.712376\n",
      "Loss at  440 3.1007953\n",
      "Loss at  450 2.831778\n",
      "Loss at  460 3.171819\n",
      "Loss at  470 2.8546834\n",
      "Loss at  480 3.0394375\n",
      "Loss at  490 2.896404\n",
      "Loss at  500 2.8665817\n",
      "Loss at  510 2.7346206\n",
      "Loss at  520 3.005366\n",
      "Loss at  530 2.9858909\n",
      "Loss at  540 3.3281593\n",
      "Loss at  550 3.4647923\n",
      "Loss at  560 2.7089918\n",
      "Loss at  570 3.4280827\n",
      "Loss at  580 2.8880303\n",
      "Loss at  590 3.1190333\n",
      "Loss at  600 2.8735857\n",
      "Loss at  610 2.8457713\n",
      "Loss at  620 2.728183\n",
      "Loss at  630 2.7699082\n",
      "Loss at  640 2.7665775\n",
      "Loss at  650 3.0704262\n",
      "Loss at  660 2.919355\n",
      "Loss at  670 2.606564\n",
      "Loss at  680 2.9698436\n",
      "Loss at  690 3.115504\n",
      "Loss at  700 2.794187\n",
      "Loss at  710 2.6707509\n",
      "Loss at  720 2.827252\n",
      "Loss at  730 3.1019874\n",
      "Loss at  740 3.0583751\n",
      "Loss at  750 3.0781503\n",
      "Loss at  760 2.6920323\n",
      "Loss at  770 2.7058632\n",
      "Loss at  780 3.1434648\n",
      "Loss at  790 3.2927513\n",
      "Loss at  800 2.9792507\n",
      "Loss at  810 2.8189597\n",
      "Loss at  820 3.2712607\n",
      "Loss at  830 3.107363\n",
      "Loss at  840 2.7164211\n",
      "Loss at  850 2.8187587\n",
      "Loss at  860 2.7776985\n",
      "Loss at  870 2.7627692\n",
      "Loss at  880 2.9208674\n",
      "Loss at  890 2.7498565\n",
      "Loss at  900 2.9920495\n",
      "Loss at  910 2.9572818\n",
      "Loss at  920 2.760516\n",
      "Loss at  930 2.7326949\n",
      "Loss at  940 2.8519163\n",
      "Loss at  950 2.7158515\n",
      "Loss at  960 3.029527\n",
      "Loss at  970 2.6662927\n",
      "Loss at  980 2.6887982\n",
      "Loss at  990 2.6191938\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Launch the graph in a session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(1000):\n",
    "        batch_inputs, batch_labels = generate_batch(BATCH_SIZE)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        batch_labels = np.expand_dims(batch_labels, axis = 1)\n",
    "        _, loss_val = sess.run([train_op, loss],\n",
    "                            feed_dict = {train_inputs:batch_inputs,\n",
    "                                         train_labels:batch_labels})\n",
    "        if step % 10 == 0:\n",
    "            print(\"Loss at \", step, loss_val) \n",
    "    #Final embeddings are ready for you to use. \n",
    "    #Need to normalize for practical use\n",
    "    trained_emb = embeddings.eval()\n",
    "    \n",
    "#Show word2vec if dim is 2\n",
    "if trained_emb.shape[1] == 2:\n",
    "    labels = rdic[:10] #Show top 10 words\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = trained_emb[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy = (x, y), xytext = (5, 2),\n",
    "                    textcoords = \"offset points\", ha = \"right\", va = \"bottom\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
